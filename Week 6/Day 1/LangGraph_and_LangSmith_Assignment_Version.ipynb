{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- ü§ù Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "- ü§ù Breakout Room #2:\n",
        "  - Part 1: LangSmith Evaluator:\n",
        "    1. Creating an Evaluation Dataset\n",
        "    2. Adding Evaluators\n",
        "  - Part 2:\n",
        "    3. Adding Helpfulness Check and \"Loop\" Limits\n",
        "    4. LangGraph for the \"Patterns\" of GenAI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# ü§ù Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## Part 1: LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "3b97db0d-d119-4b43-b964-47291c7dba1c"
      },
      "outputs": [],
      "source": [
        "!pip install -qU langchain langchain_openai langchain-community langgraph arxiv duckduckgo_search==5.3.1b1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "761167a9-b570-421b-eb9c-8be3dc813f47"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "b0237b19-ada7-4836-edc2-228e694a5ecb"
      },
      "outputs": [],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE3 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####üèóÔ∏è Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [\n",
        "    DuckDuckGoSearchRun(),\n",
        "    ArxivQueryRun()\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/fab950acfbf5fea46c9313dca34ee2ae01f1728b/libs/langgraph/langgraph/prebuilt/tool_executor.py#L50) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ‚ùì Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "#### ANSWER: The LLM outputs the payload for the selected tool based on the prompt, the descriptions of the tools, and the inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Task 4: Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated\n",
        "from langgraph.graph.message import add_messages\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## Task 5: It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ‚ùì Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "\n",
        "#### ANSWER: Recursion limit as a parameter helps limit the number of cycles, you could also manage this with state and a conditional edge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GSCds6zTL5VJ"
      },
      "source": [
        "#### Helper Function to print messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "xRPF0X5iL8Bh"
      },
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "  next_is_tool = False\n",
        "  initial_query = True\n",
        "  for message in messages[\"messages\"]:\n",
        "    if \"function_call\" in message.additional_kwargs:\n",
        "      print()\n",
        "      print(f'Tool Call - Name: {message.additional_kwargs[\"function_call\"][\"name\"]} + Query: {message.additional_kwargs[\"function_call\"][\"arguments\"]}')\n",
        "      next_is_tool = True\n",
        "      continue\n",
        "    if next_is_tool:\n",
        "      print(f\"Tool Response: {message.content}\")\n",
        "      next_is_tool = False\n",
        "      continue\n",
        "    if initial_query:\n",
        "      print(f\"Initial Query: {message.content}\")\n",
        "      print()\n",
        "      initial_query = False\n",
        "      continue\n",
        "    print()\n",
        "    print(f\"Agent Response: {message.content}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "90f7d3dc-0fe2-4d1f-8221-9b3bcffc982e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is RAG in the context of Large Language Models? When did it break onto the scene?\n",
            "\n",
            "\n",
            "Agent Response: RAG, or Retrieval-Augmented Generation, is a technique used in the context of large language models (LLMs) to improve their performance by combining retrieval-based methods with generative models. The core idea behind RAG is to enhance the generative capabilities of language models by incorporating relevant information retrieved from a large corpus of documents or a knowledge base. This approach helps the model generate more accurate and contextually relevant responses, especially for tasks that require specific knowledge or up-to-date information.\n",
            "\n",
            "### Key Components of RAG:\n",
            "1. **Retriever**: This component is responsible for fetching relevant documents or passages from a large corpus based on the input query.\n",
            "2. **Generator**: This is typically a generative language model (like GPT-3) that uses the retrieved documents to generate a coherent and contextually appropriate response.\n",
            "\n",
            "### How RAG Works:\n",
            "1. **Query Input**: The user provides a query or prompt.\n",
            "2. **Retrieval**: The retriever searches a large corpus to find documents or passages that are relevant to the query.\n",
            "3. **Generation**: The generator uses the retrieved documents as additional context to generate a response to the query.\n",
            "\n",
            "### Advantages of RAG:\n",
            "- **Improved Accuracy**: By leveraging external documents, the model can provide more accurate and detailed responses.\n",
            "- **Up-to-date Information**: The retriever can access the latest information, making the model's responses more current.\n",
            "- **Contextual Relevance**: The model can generate responses that are more contextually relevant to the user's query.\n",
            "\n",
            "### When Did RAG Break Onto the Scene?\n",
            "RAG was introduced in a research paper by Facebook AI (now Meta AI) in 2020. The paper, titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" was published on arXiv in August 2020. The introduction of RAG marked a significant advancement in the field of natural language processing (NLP) by combining the strengths of retrieval-based and generative models.\n",
            "\n",
            "Would you like more detailed information or specific aspects of RAG?\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "6ae3afec-7da7-4c5d-a3ca-2ed75735c029"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initial Query: What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\n",
            "\n",
            "\n",
            "Tool Call - Name: arxiv + Query: {\"query\":\"QLoRA machine learning\"}\n",
            "Tool Response: Published: 2023-05-23\n",
            "Title: QLoRA: Efficient Finetuning of Quantized LLMs\n",
            "Authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "Summary: We present QLoRA, an efficient finetuning approach that reduces memory usage\n",
            "enough to finetune a 65B parameter model on a single 48GB GPU while preserving\n",
            "full 16-bit finetuning task performance. QLoRA backpropagates gradients through\n",
            "a frozen, 4-bit quantized pretrained language model into Low Rank\n",
            "Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\n",
            "previous openly released models on the Vicuna benchmark, reaching 99.3% of the\n",
            "performance level of ChatGPT while only requiring 24 hours of finetuning on a\n",
            "single GPU. QLoRA introduces a number of innovations to save memory without\n",
            "sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\n",
            "information theoretically optimal for normally distributed weights (b) double\n",
            "quantization to reduce the average memory footprint by quantizing the\n",
            "quantization constants, and (c) paged optimziers to manage memory spikes. We\n",
            "use QLoRA to finetune more than 1,000 models, providing a detailed analysis of\n",
            "instruction following and chatbot performance across 8 instruction datasets,\n",
            "multiple model types (LLaMA, T5), and model scales that would be infeasible to\n",
            "run with regular finetuning (e.g. 33B and 65B parameter models). Our results\n",
            "show that QLoRA finetuning on a small high-quality dataset leads to\n",
            "state-of-the-art results, even when using smaller models than the previous\n",
            "SoTA. We provide a detailed analysis of chatbot performance based on both human\n",
            "and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\n",
            "alternative to human evaluation. Furthermore, we find that current chatbot\n",
            "benchmarks are not trustworthy to accurately evaluate the performance levels of\n",
            "chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\n",
            "ChatGPT. We release all of our models and code, including CUDA kernels for\n",
            "4-bit training.\n",
            "\n",
            "Published: 2024-05-27\n",
            "Title: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\n",
            "Authors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "Summary: The LoRA-finetuning quantization of LLMs has been extensively studied to\n",
            "obtain accurate yet compact LLMs for deployment on resource-constrained\n",
            "hardware. However, existing methods cause the quantized LLM to severely degrade\n",
            "and even fail to benefit from the finetuning of LoRA. This paper proposes a\n",
            "novel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\n",
            "through information retention. The proposed IR-QLoRA mainly relies on two\n",
            "technologies derived from the perspective of unified information: (1)\n",
            "statistics-based Information Calibration Quantization allows the quantized\n",
            "parameters of LLM to retain original information accurately; (2)\n",
            "finetuning-based Information Elastic Connection makes LoRA utilizes elastic\n",
            "representation transformation with diverse information. Comprehensive\n",
            "experiments show that IR-QLoRA can significantly improve accuracy across LLaMA\n",
            "and LLaMA2 families under 2-4 bit-widths, e.g., 4- bit LLaMA-7B achieves 1.4%\n",
            "improvement on MMLU compared with the state-of-the-art methods. The significant\n",
            "performance gain requires only a tiny 0.31% additional time consumption,\n",
            "revealing the satisfactory efficiency of our IR-QLoRA. We highlight that\n",
            "IR-QLoRA enjoys excellent versatility, compatible with various frameworks\n",
            "(e.g., NormalFloat and Integer quantization) and brings general accuracy gains.\n",
            "The code is available at https://github.com/htqin/ir-qlora.\n",
            "\n",
            "Published: 2024-02-16\n",
            "Title: QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning\n",
            "Authors: Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh\n",
            "Summary: Finetuning large language models requires huge GPU memory, restricting the\n",
            "choice to ac\n",
            "\n",
            "Tool Call - Name: duckduckgo_search + Query: {\"query\":\"Tim Dettmers biography\"}\n",
            "Tool Response: Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Speaker Biography. Tim Dettmers' research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers is a graduating PhD student advised by Luke Zettlemoyer at the University of Washington in Seattle. He holds degrees in applied math and computer science and has a background in industrial automation. His primary research goal is to democratize foundation models by making them more efficient and accessible through quantization ...\n",
            "\n",
            "Agent Response: ### What is QLoRA in Machine Learning?\n",
            "\n",
            "QLoRA (Quantized Low-Rank Adaptation) is an efficient finetuning approach designed to reduce memory usage while preserving the performance of large language models (LLMs). The key features of QLoRA include:\n",
            "\n",
            "1. **Memory Efficiency**: It allows finetuning of a 65 billion parameter model on a single 48GB GPU.\n",
            "2. **Quantization**: Utilizes 4-bit quantization of pretrained language models to save memory.\n",
            "3. **Low-Rank Adapters (LoRA)**: Backpropagates gradients through frozen, quantized models into low-rank adapters.\n",
            "4. **Innovations**:\n",
            "   - **4-bit NormalFloat (NF4)**: A new data type optimized for normally distributed weights.\n",
            "   - **Double Quantization**: Reduces memory footprint by quantizing the quantization constants.\n",
            "   - **Paged Optimizers**: Manages memory spikes during training.\n",
            "\n",
            "The approach has been shown to achieve state-of-the-art results with smaller models and less computational resources, making it a significant advancement in the field of machine learning.\n",
            "\n",
            "### Technical Papers on QLoRA\n",
            "\n",
            "1. **Title**: [QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\n",
            "   - **Summary**: This paper introduces QLoRA and demonstrates its effectiveness in finetuning large language models with reduced memory usage while maintaining high performance.\n",
            "\n",
            "2. **Title**: [Accurate LoRA-Finetuning Quantization of LLMs via Information Retention](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\n",
            "   - **Summary**: This paper proposes IR-QLoRA, an improved version of QLoRA that focuses on information retention to enhance the accuracy of quantized LLMs.\n",
            "\n",
            "3. **Title**: [QDyLoRA: Quantized Dynamic Low-Rank Adaptation for Efficient Large Language Model Tuning](https://arxiv.org/abs/2305.14314)\n",
            "   - **Authors**: Hossein Rajabzadeh, Mojtaba Valipour, Tianshu Zhu, Marzieh Tahaei, Hyock Ju Kwon, Ali Ghodsi, Boxing Chen, Mehdi Rezagholizadeh\n",
            "   - **Summary**: This paper discusses QDyLoRA, which combines dynamic low-rank adaptation with quantization for efficient tuning of large language models.\n",
            "\n",
            "### Biography of Tim Dettmers\n",
            "\n",
            "Tim Dettmers is a PhD student at the University of Washington in Seattle, advised by Luke Zettlemoyer. He holds degrees in applied mathematics and computer science and has a background in industrial automation. His research focuses on making foundation models, such as ChatGPT, more accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cost-effective deep learning. His primary research goal is to democratize foundation models by making them more efficient and accessible through quantization and other techniques.\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any technical papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "messages = app.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####üèóÔ∏è Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer.\n",
        "\n",
        "\n",
        "### ANSWER\n",
        "\n",
        "\n",
        "- Agents recives querry\n",
        "- Agent searches ArXiV, and it retrives 3 papers\n",
        "- Agent uses DuckDuckGO to search for biographies of the authors\n",
        "- create answer based on 2 previous calls\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# ü§ù Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7c8-Uyarh1v"
      },
      "source": [
        "## Part 1: LangSmith Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "d2b3be9d-1dbb-4109-83e4-673c9b73483e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-Augmented Generation. It is a technique used in natural language processing (NLP) and machine learning to improve the performance of language models by combining retrieval-based methods with generative models. Here‚Äôs a brief overview of how it works:\\n\\n1. **Retrieval**: In the first step, the system retrieves relevant documents or pieces of information from a large corpus based on the input query. This is typically done using a retrieval model, such as BM25 or a dense retrieval model like DPR (Dense Passage Retrieval).\\n\\n2. **Augmentation**: The retrieved documents are then used to augment the input query. This can involve concatenating the retrieved information with the original query or using it to provide additional context.\\n\\n3. **Generation**: Finally, a generative model, such as a transformer-based language model (e.g., GPT-3, BERT), uses the augmented input to generate a response. The generative model can produce more accurate and contextually relevant answers by leveraging the additional information provided by the retrieval step.\\n\\nRAG models are particularly useful in scenarios where the information needed to answer a query is not contained within the model's parameters but can be found in external documents. This approach helps in generating more accurate and informative responses, especially for complex queries that require specific knowledge.\""
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "### Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####üèóÔ∏è Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ‚ùì Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "#### ANSWER: Simply by element - there are potential ordering issues that could develop for more complex systems - using metadata would improve the system tremendously"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "### Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ‚ùì Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "#### ANSWER: This does not take into account case, spelling mistakes, or potential additional context."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        "    eval_llm=ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "157f1019-4ee4-4994-fe80-7afc2e5fdfb6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 85ceb833' at:\n",
            "https://smith.langchain.com/o/5e6ca0fb-60fe-5c40-bc3d-aa6ed4eb0812/datasets/1c487daa-2a95-4062-a9e3-59757197ad49/compare?selectedSessions=29d7f41e-616f-456f-8e9a-28440a1d5033\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 2325fa79 at:\n",
            "https://smith.langchain.com/o/5e6ca0fb-60fe-5c40-bc3d-aa6ed4eb0812/datasets/1c487daa-2a95-4062-a9e3-59757197ad49\n",
            "[------------------------------------------------->] 6/6"
          ]
        },
        {
          "data": {
            "text/html": [
              "<h3>Experiment Results:</h3>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>feedback.helpfulness</th>\n",
              "      <th>feedback.COT Contextual Accuracy</th>\n",
              "      <th>feedback.must_mention</th>\n",
              "      <th>error</th>\n",
              "      <th>execution_time</th>\n",
              "      <th>run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>True</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>90d57e9e-925b-4134-b7ff-02b2fee7653c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.833333</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.636953</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.408248</td>\n",
              "      <td>0.408248</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>1.795195</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>3.789788</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>4.180712</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>5.246345</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.129874</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>7.956024</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        feedback.helpfulness  feedback.COT Contextual Accuracy  \\\n",
              "count               6.000000                          6.000000   \n",
              "unique                   NaN                               NaN   \n",
              "top                      NaN                               NaN   \n",
              "freq                     NaN                               NaN   \n",
              "mean                0.833333                          0.833333   \n",
              "std                 0.408248                          0.408248   \n",
              "min                 0.000000                          0.000000   \n",
              "25%                 1.000000                          1.000000   \n",
              "50%                 1.000000                          1.000000   \n",
              "75%                 1.000000                          1.000000   \n",
              "max                 1.000000                          1.000000   \n",
              "\n",
              "       feedback.must_mention error  execution_time  \\\n",
              "count                      6     0        6.000000   \n",
              "unique                     2     0             NaN   \n",
              "top                     True   NaN             NaN   \n",
              "freq                       4   NaN             NaN   \n",
              "mean                     NaN   NaN        5.636953   \n",
              "std                      NaN   NaN        1.795195   \n",
              "min                      NaN   NaN        3.789788   \n",
              "25%                      NaN   NaN        4.180712   \n",
              "50%                      NaN   NaN        5.246345   \n",
              "75%                      NaN   NaN        7.129874   \n",
              "max                      NaN   NaN        7.956024   \n",
              "\n",
              "                                      run_id  \n",
              "count                                      6  \n",
              "unique                                     6  \n",
              "top     90d57e9e-925b-4134-b7ff-02b2fee7653c  \n",
              "freq                                       1  \n",
              "mean                                     NaN  \n",
              "std                                      NaN  \n",
              "min                                      NaN  \n",
              "25%                                      NaN  \n",
              "50%                                      NaN  \n",
              "75%                                      NaN  \n",
              "max                                      NaN  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 85ceb833',\n",
              " 'results': {'9075c03d-1aaa-4b04-9d56-ed65ce4ccfd7': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='To determine if the submission meets the criteria of helpfulness, I will evaluate it based on the following aspects:\\n\\n1. **Helpfulness**: Does the submission provide a clear and accurate answer to the question asked?\\n2. **Insightfulness**: Does the submission offer additional useful information that enhances understanding?\\n3. **Appropriateness**: Is the submission relevant and directly addresses the input question?\\n\\n### Step-by-Step Evaluation:\\n\\n1. **Helpfulness**:\\n   - The input question asks specifically about the optimizer used in QLoRA.\\n   - The submission mentions \"paged optimizers\" as a key innovation in QLoRA for managing memory spikes.\\n   - The submission explains that this approach allows fine-tuning large models on a single GPU while preserving performance.\\n   - However, the submission does not explicitly state the specific optimizer used in QLoRA, which is Adam.\\n\\n2. **Insightfulness**:\\n   - The submission provides additional context about the use of paged optimizers and their benefits in memory management.\\n   - This information is useful for understanding the broader context of QLoRA\\'s optimization strategy.\\n\\n3. **Appropriateness**:\\n   - The submission is relevant to the input question as it discusses optimization techniques used in QLoRA.\\n   - However, it does not directly answer the specific question about the optimizer used, which is a critical detail.\\n\\n### Conclusion:\\nWhile the submission provides useful and relevant information about QLoRA\\'s optimization techniques, it fails to directly answer the specific question about the optimizer used. Therefore, it does not fully meet the criteria of being helpful.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9f32dab1-d55f-4cb3-9fb7-4f1b017e24e0'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment='EXPLANATION:\\n1. The question asks specifically about the optimizer used in QLoRA.\\n2. The context provided includes the terms \\'paged\\' and \\'optimizer\\', suggesting that the correct answer should involve these concepts.\\n3. The student\\'s answer explains that QLoRA uses \"paged optimizers\" to manage memory spikes during the fine-tuning of large language models.\\n4. The student\\'s answer aligns with the context by mentioning \"paged optimizers\" and explaining their role in memory management.\\n5. There are no conflicting statements in the student\\'s answer, and it accurately reflects the information given in the context.\\n\\nGRADE: CORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6e9350ea-3beb-42d9-b278-d9c7a33ec1f7'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('0f2127f9-321d-4195-afec-428469917354'), target_run_id=None)],\n",
              "   'execution_time': 4.581583,\n",
              "   'run_id': '90d57e9e-925b-4134-b7ff-02b2fee7653c',\n",
              "   'output': 'QLoRA (Quantized Low-Rank Adaptation) uses a combination of techniques to optimize memory usage and performance during the fine-tuning of large language models. One of the key innovations mentioned in the QLoRA paper is the use of \"paged optimizers\" to manage memory spikes. This approach allows the fine-tuning of large models, such as a 65B parameter model, on a single 48GB GPU while preserving full 16-bit fine-tuning task performance. \\n\\nIn summary, QLoRA employs paged optimizers as part of its strategy to efficiently manage memory during the fine-tuning process.',\n",
              "   'reference': {'must_mention': ['paged', 'optimizer']}},\n",
              "  '4db551cc-f790-44c1-9066-12e9ff8c7783': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='To determine if the submission meets the criteria of helpfulness, I will evaluate it based on the following aspects:\\n\\n1. **Helpfulness**: Does the submission provide a clear and accurate answer to the input question?\\n2. **Insightfulness**: Does the submission offer additional useful information that enhances understanding?\\n3. **Appropriateness**: Is the submission relevant and suitable for the context of the question?\\n\\n### Step-by-Step Evaluation:\\n\\n1. **Helpfulness**:\\n   - The input question asks about the data type created in the QLoRA paper.\\n   - The submission states that the data type is **4-bit NormalFloat (NF4)**.\\n   - This directly answers the question, providing the specific name of the data type.\\n\\n2. **Insightfulness**:\\n   - The submission goes beyond just naming the data type; it explains that NF4 is designed to be information-theoretically optimal for normally distributed weights.\\n   - It also mentions the benefits of this data type, such as reducing memory usage while preserving performance during the finetuning of large language models.\\n   - This additional information helps in understanding the significance and application of the data type.\\n\\n3. **Appropriateness**:\\n   - The submission is relevant to the question asked.\\n   - It provides a concise and accurate response without deviating from the topic.\\n\\nBased on the above evaluation, the submission meets all the criteria of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('61d5e0de-74a1-4b7e-84d3-e4bef00beb38'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment='EXPLANATION:\\n1. Identify the key information in the context: The context mentions two data types, \\'NF4\\' and \\'NormalFloat\\'.\\n2. Analyze the student\\'s answer: The student states that a new data type called \"4-bit NormalFloat (NF4)\" was introduced in the QLoRA paper.\\n3. Compare the student\\'s answer with the context: The student\\'s answer correctly identifies \\'NF4\\' as the data type introduced in the QLoRA paper. The additional information provided by the student about the data type being designed for normally distributed weights and its benefits does not conflict with the context.\\n4. Determine if the student\\'s answer is factually accurate based on the context: The student\\'s answer is consistent with the context provided.\\n\\nGRADE: CORRECT', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a548a5e6-d992-47fe-94a9-60fa8812d273'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('01425d65-bc4c-4f0c-af49-d75122f4b5c4'), target_run_id=None)],\n",
              "   'execution_time': 3.789788,\n",
              "   'run_id': 'b13c0ed0-35e7-484e-b817-6c9ac4dec874',\n",
              "   'output': 'In the QLoRA paper, a new data type called **4-bit NormalFloat (NF4)** was introduced. This data type is designed to be information-theoretically optimal for normally distributed weights, which helps in reducing memory usage while preserving performance during the finetuning of large language models.',\n",
              "   'reference': {'must_mention': ['NF4', 'NormalFloat']}},\n",
              "  '9cb97f21-9d15-486e-975f-c8fd74a44047': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"To determine if the submission meets the criteria of helpfulness, insightfulness, and appropriateness, let's break down each aspect:\\n\\n1. **Helpfulness**:\\n   - The submission provides a clear and detailed explanation of what a Retrieval-Augmented Generation (RAG) system is.\\n   - It breaks down the two main components (retrieval-based and generation-based) and explains their roles.\\n   - It outlines the process of how a RAG system works step-by-step.\\n   - It lists the benefits of using a RAG system.\\n   - It provides examples of applications where RAG systems can be useful.\\n\\n2. **Insightfulness**:\\n   - The submission goes beyond a basic definition by explaining the mechanisms and benefits of RAG systems.\\n   - It mentions specific techniques and models used in the retrieval component (e.g., TF-IDF, BM25, Dense Passage Retrieval).\\n   - It highlights the flexibility and various applications of RAG systems, showing a deep understanding of the topic.\\n\\n3. **Appropriateness**:\\n   - The language and terminology used are appropriate for someone seeking to understand what a RAG system is.\\n   - The submission is structured in a logical and easy-to-follow manner.\\n   - It provides relevant and accurate information without unnecessary jargon or complexity.\\n\\nBased on the above analysis, the submission is helpful, insightful, and appropriate. Therefore, it meets the criteria.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b1f913a6-8da6-4d07-9437-fb5017da8e74'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"EXPLANATION:\\n1. **Identify the Key Elements of the Question**: The question asks for an explanation of what a Retrieval Augmented Generation (RAG) system is.\\n2. **Review the Context**: The context provided is 'ground' and 'context', which are not very specific. However, we can infer that the context is about understanding the concept of a RAG system.\\n3. **Analyze the Student's Answer**:\\n   - The student defines a RAG system as a machine learning model that combines retrieval-based and generation-based approaches.\\n   - The student breaks down the two components:\\n     - **Retrieval-Based Component**: Describes how the system retrieves relevant documents using techniques like TF-IDF, BM25, or neural retrieval models.\\n     - **Generation-Based Component**: Describes how the system generates text using models like GPT-3 or BERT.\\n   - The student explains the working of RAG:\\n     - Input Query\\n     - Retrieval\\n     - Augmentation\\n     - Generation\\n   - The student lists the benefits of RAG:\\n     - Improved Relevance\\n     - Knowledge Integration\\n     - Flexibility\\n   - The student provides applications of RAG:\\n     - Question Answering\\n     - Conversational AI\\n     - Content Creation\\n4. **Check for Factual Accuracy**:\\n   - The student's explanation of the components, working, benefits, and applications of RAG is accurate and aligns with the general understanding of RAG systems.\\n   - There are no conflicting statements or inaccuracies in the student's answer.\\n5. **Conclusion**: The student's answer is comprehensive and factually correct based on the context of explaining what a RAG system is.\\n\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cac4ed2b-6439-4fd9-81e2-1a483718e784'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('ed6c4f6c-5153-43a3-9cee-f1457dd94a06'), target_run_id=None)],\n",
              "   'execution_time': 7.53613,\n",
              "   'run_id': '14f9f54c-98dd-446d-8efb-807b52acf9c5',\n",
              "   'output': 'A Retrieval-Augmented Generation (RAG) system is a type of machine learning model that combines retrieval-based and generation-based approaches to improve the quality and relevance of generated text. Here‚Äôs a breakdown of the two components:\\n\\n1. **Retrieval-Based Component**:\\n   - This part of the system retrieves relevant documents or pieces of information from a large corpus or database based on the input query. The retrieval mechanism typically uses techniques from information retrieval, such as TF-IDF, BM25, or more advanced neural retrieval models like Dense Passage Retrieval (DPR).\\n\\n2. **Generation-Based Component**:\\n   - This part of the system generates text based on the retrieved documents and the input query. It usually employs a generative model, such as a Transformer-based model (e.g., GPT-3, BERT), to produce coherent and contextually relevant text.\\n\\n### How RAG Works:\\n1. **Input Query**: The system receives an input query or prompt.\\n2. **Retrieval**: The retrieval component searches a large corpus to find documents or passages that are relevant to the query.\\n3. **Augmentation**: The retrieved documents are then used to augment the input query, providing additional context and information.\\n4. **Generation**: The augmented query is fed into the generative model, which produces the final output text.\\n\\n### Benefits of RAG:\\n- **Improved Relevance**: By leveraging external documents, the system can generate more accurate and contextually relevant responses.\\n- **Knowledge Integration**: It can incorporate up-to-date information from a large corpus, making it useful for tasks that require current knowledge.\\n- **Flexibility**: It can be applied to various tasks, including question answering, summarization, and conversational agents.\\n\\n### Applications:\\n- **Question Answering**: Providing detailed and accurate answers by retrieving relevant documents and generating a coherent response.\\n- **Conversational AI**: Enhancing chatbot responses with up-to-date and contextually relevant information.\\n- **Content Creation**: Assisting in writing articles, reports, or summaries by retrieving and integrating relevant information.\\n\\nOverall, RAG systems represent a powerful approach to combining the strengths of retrieval and generation, leading to more effective and informative text generation.',\n",
              "   'reference': {'must_mention': ['ground', 'context']}},\n",
              "  '6e834794-ff5c-4211-8bb8-a05839ca0df6': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"To determine if the submission meets the criteria of helpfulness, insightfulness, and appropriateness, let's break down each aspect:\\n\\n1. **Helpfulness**:\\n   - The submission provides the names of the authors of the QLoRA paper, which directly answers the input question.\\n   - It also includes the title of the paper, which adds context and confirms the specific paper being referred to.\\n   - The publication date is an additional piece of information that, while not requested, could be useful for context.\\n\\n2. **Insightfulness**:\\n   - The submission goes beyond just listing the authors by including the title and publication date, which shows a deeper engagement with the content.\\n   - This additional information can help the reader verify the details and understand the context better.\\n\\n3. **Appropriateness**:\\n   - The submission is relevant to the input question and stays on topic.\\n   - The language used is clear and professional, making it easy to understand.\\n\\nGiven this analysis, the submission is helpful because it directly answers the question and provides additional useful context. It is insightful as it includes extra details that enhance understanding. It is appropriate as it stays on topic and uses clear language.\\n\\nTherefore, the submission meets all the criteria.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1c4b0662-0ca8-4764-b9f8-63286bfbf774'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"1. **Identify the key information in the question**: The question asks for the author(s) of the QLoRA paper.\\n\\n2. **Review the context provided**: The context lists two names: 'Tim' and 'Dettmers'. This suggests that Tim Dettmers is an author of the QLoRA paper.\\n\\n3. **Analyze the student's answer**: The student lists four authors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Additionally, the student mentions the paper's title and publication date.\\n\\n4. **Compare the student's answer to the context**: The context only confirms Tim Dettmers as an author. The student's answer includes additional names not mentioned in the context.\\n\\n5. **Determine if the additional information conflicts with the context**: The additional names (Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer) are not mentioned in the context, but they do not contradict the context either. The context does not state that Tim Dettmers is the sole author.\\n\\n6. **Evaluate the relevance of the additional information**: The additional information about the paper's title and publication date is not relevant to the question of authorship but does not conflict with the context.\\n\\n7. **Conclusion**: The student's answer includes Tim Dettmers, which matches the context. The additional authors and information do not contradict the context.\\n\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1bc6a416-e81d-4b10-b6a4-dbe198e93f3a'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('afc258f4-92a3-4e16-806b-0d72931dc1eb'), target_run_id=None)],\n",
              "   'execution_time': 4.047088,\n",
              "   'run_id': '7d4887e8-82a3-4349-91df-88dfca1f4c22',\n",
              "   'output': 'The QLoRA paper titled \"QLoRA: Efficient Finetuning of Quantized LLMs\" was authored by:\\n\\n- Tim Dettmers\\n- Artidoro Pagnoni\\n- Ari Holtzman\\n- Luke Zettlemoyer\\n\\nThe paper was published on May 23, 2023.',\n",
              "   'reference': {'must_mention': ['Tim', 'Dettmers']}},\n",
              "  'd395e73a-4294-4b49-8b96-9bae19b0820c': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"To determine if the submission meets the criteria of helpfulness, I will evaluate it based on the following aspects:\\n\\n1. **Helpfulness**: Does the submission provide useful information in response to the input question?\\n   - The input question asks for the most popular deep learning framework.\\n   - The submission lists three popular deep learning frameworks: TensorFlow, PyTorch, and Keras.\\n   - It provides a brief description of each framework, including their developers, key features, and areas of popularity.\\n\\n2. **Insightfulness**: Does the submission offer insightful information that adds value to the reader's understanding?\\n   - The submission explains why each framework is popular, mentioning aspects like community support, ease of use, and specific features (e.g., dynamic computation graph for PyTorch).\\n   - It also notes the adoption of these frameworks in both industry and academia, which adds context to their popularity.\\n\\n3. **Appropriateness**: Is the submission appropriate in terms of relevance and accuracy?\\n   - The submission is relevant to the input question as it directly addresses the popularity of deep learning frameworks.\\n   - The information provided is accurate and up-to-date as of 2023, mentioning well-known facts about TensorFlow, PyTorch, and Keras.\\n\\nBased on this evaluation, the submission is helpful, insightful, and appropriate. Therefore, it meets the criteria.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('05f78dd7-887c-454d-a38b-eed835fe4648'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=0, value='INCORRECT', comment=\"EXPLANATION:\\n1. The question asks for the most popular deep learning framework.\\n2. The context provided lists two frameworks: 'PyTorch' and 'TensorFlow'.\\n3. The student's answer lists three frameworks: TensorFlow, PyTorch, and Keras.\\n4. The student's answer provides detailed descriptions of each framework.\\n5. The student's answer does not explicitly state which one is the most popular, but it does mention that TensorFlow and PyTorch are widely used.\\n6. The inclusion of Keras, while informative, does not directly address the question of which single framework is the most popular.\\n7. The student's answer does not conflict with the context but fails to directly answer the question.\\n\\nGRADE: INCORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c465fa20-ad56-4f40-bbe1-9163a18e7043'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=True, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('bbb84f80-b210-4557-ad3a-ae830750bd77'), target_run_id=None)],\n",
              "   'execution_time': 5.911106,\n",
              "   'run_id': '1b7b0e7e-a18c-41c1-b5dc-55849b2b0938',\n",
              "   'output': \"As of 2023, the most popular deep learning frameworks are:\\n\\n1. **TensorFlow**: Developed by Google, TensorFlow is one of the most widely used open-source libraries for numerical computation and deep learning. It is known for its versatility and extensive community support.\\n\\n2. **PyTorch**: Developed by Facebook's AI Research lab, PyTorch is particularly popular in the research community. It is known for its ease of use, dynamic computation graph, and strong GPU support.\\n\\n3. **Keras**: Initially developed by Fran√ßois Chollet, Keras is a high-level neural network API written in Python. It runs on top of TensorFlow, Theano, and CNTK, and is known for its simplicity and ease of use.\\n\\nThese frameworks are widely adopted in both industry and academia for various applications, including computer vision, natural language processing, and reinforcement learning.\",\n",
              "   'reference': {'must_mention': ['PyTorch', 'TensorFlow']}},\n",
              "  'ae657f60-b6c5-4797-9b78-650015c93cea': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"To determine if the submission meets the criteria of helpfulness, insightfulness, and appropriateness, let's break down each aspect:\\n\\n1. **Helpfulness**:\\n   - The submission provides a detailed explanation of the significant improvements made by the LoRA system.\\n   - It lists eight specific benefits, each with a clear and concise explanation.\\n   - The information is relevant to the input question, addressing the improvements of the LoRA system directly.\\n\\n2. **Insightfulness**:\\n   - The submission goes beyond a superficial explanation by detailing how LoRA achieves parameter efficiency, reduced computational cost, memory efficiency, faster training, and other benefits.\\n   - It provides context on why these improvements are significant, such as the impact on hardware requirements and the ability to maintain model performance.\\n   - The explanation of the modularity and flexibility of LoRA adds depth to the understanding of its advantages.\\n\\n3. **Appropriateness**:\\n   - The language used is professional and suitable for an audience interested in machine learning.\\n   - The submission stays on topic and does not include irrelevant information.\\n   - It is structured in a way that is easy to follow, with each point clearly delineated.\\n\\nGiven this analysis, the submission is helpful, insightful, and appropriate. Therefore, it meets the criteria.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('825d8644-d14c-4fa8-9cc7-85a9b06b3602'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"EXPLANATION:\\n1. The question asks about the significant improvements made by the LoRA system.\\n2. The context provided is ['reduce', 'parameters'], which suggests that the improvements should be related to reducing parameters.\\n3. The student's answer lists several improvements and benefits of the LoRA system, including:\\n   - Parameter Efficiency: LoRA allows for fine-tuning with fewer parameters by introducing low-rank matrices.\\n   - Reduced Computational Cost: Fewer parameters to update means lower computational resources required.\\n   - Memory Efficiency: Low-rank matrices consume less memory.\\n   - Faster Training: Fewer parameters lead to quicker training processes.\\n   - Maintaining Model Performance: Despite fewer parameters, performance is maintained or improved.\\n   - Flexibility: Applicable to various neural network architectures.\\n   - Scalability: Can handle fine-tuning of very large models.\\n   - Modularity: Low-rank matrices can be easily added or removed.\\n\\n4. The student's answer aligns with the context of reducing parameters and provides detailed explanations of how LoRA achieves this and the benefits it brings.\\n5. There are no conflicting statements in the student's answer, and it accurately reflects the improvements related to reducing parameters.\\n\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('af69150f-50e2-4207-b4a4-4281557e9ecd'))}, feedback_config=None, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, feedback_config=None, source_run_id=UUID('39c87db0-b7c0-4620-a5f5-b35d1059c955'), target_run_id=None)],\n",
              "   'execution_time': 7.956024,\n",
              "   'run_id': '1cae83e1-8e26-4d7a-b923-74216d7430a6',\n",
              "   'output': 'LoRA, or Low-Rank Adaptation, is a technique used in the field of machine learning, particularly in the context of fine-tuning large language models. Here are some significant improvements and benefits that the LoRA system brings:\\n\\n1. **Parameter Efficiency**: LoRA allows for fine-tuning large models with a significantly smaller number of parameters. Instead of updating all the parameters of the model, LoRA introduces low-rank matrices that are much smaller in size, making the fine-tuning process more efficient.\\n\\n2. **Reduced Computational Cost**: By reducing the number of parameters that need to be updated, LoRA decreases the computational resources required for fine-tuning. This makes it feasible to fine-tune large models on smaller hardware setups.\\n\\n3. **Memory Efficiency**: The low-rank matrices used in LoRA consume less memory compared to the full set of model parameters. This is particularly beneficial when working with very large models that would otherwise require substantial memory resources.\\n\\n4. **Faster Training**: With fewer parameters to update, the training process can be faster. This allows for quicker iterations and experimentation, which is valuable in research and development settings.\\n\\n5. **Maintaining Model Performance**: Despite the reduction in the number of parameters, LoRA has been shown to maintain, and sometimes even improve, the performance of the fine-tuned models. This is achieved by effectively capturing the essential information needed for the specific task.\\n\\n6. **Flexibility**: LoRA can be applied to various types of neural networks and is not limited to a specific architecture. This makes it a versatile tool for different machine learning applications.\\n\\n7. **Scalability**: As models continue to grow in size, the scalability of fine-tuning methods becomes increasingly important. LoRA provides a scalable solution that can handle the fine-tuning of very large models without prohibitive resource requirements.\\n\\n8. **Modularity**: The low-rank matrices introduced by LoRA can be easily added or removed, making the system modular. This allows for easy experimentation with different configurations and adaptations.\\n\\nOverall, LoRA represents a significant advancement in the efficient fine-tuning of large language models, making it a valuable tool for both researchers and practitioners in the field of machine learning.',\n",
              "   'reference': {'must_mention': ['reduce', 'parameters']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhTNe4kWrplB"
      },
      "source": [
        "## Part 2: LangGraph with Helpfulness:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1wKRddbIY_S"
      },
      "source": [
        "### Task 3: Adding Helpfulness Check and \"Loop\" Limits\n",
        "\n",
        "Now that we've done evaluation - let's see if we can add an extra step where we review the content we've generated to confirm if it fully answers the user's query!\n",
        "\n",
        "We're going to make a few key adjustments to account for this:\n",
        "\n",
        "1. We're going to add an artificial limit on how many \"loops\" the agent can go through - this will help us to avoid the potential situation where we never exit the loop.\n",
        "2. We'll add a custom node and conditional edge to determine if the response was helpful enough."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npTYJ8ayR5B3"
      },
      "source": [
        "First, let's define our state again - we can check the length of the state object, so we don't need additional state for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "-LQ84YhyJG0w"
      },
      "outputs": [],
      "source": [
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[list, add_messages]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gC8t-4FISCEh"
      },
      "source": [
        "We're going to add a custom helpfulness check here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ZV_PxI5zNY7f"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "def check_helpfulness(state):\n",
        "  initial_query = state[\"messages\"][0]\n",
        "  final_response = state[\"messages\"][-1]\n",
        "\n",
        "  if len(state[\"messages\"]) > 10:\n",
        "    return \"END\"\n",
        "\n",
        "  prompt_template = \"\"\"\\\n",
        "  Given an initial query and a final response, determine if the final response is extremely helpful or not. Please indicate helpfulness with a 'Y' and unhelpfulness as an 'N'.\n",
        "\n",
        "  Initial Query:\n",
        "  {initial_query}\n",
        "\n",
        "  Final Response:\n",
        "  {final_response}\"\"\"\n",
        "\n",
        "  prompt_template = PromptTemplate.from_template(prompt_template)\n",
        "\n",
        "  helpfulness_check_model = ChatOpenAI(model=\"gpt-4\")\n",
        "\n",
        "  helpfulness_chain = prompt_template | helpfulness_check_model | StrOutputParser()\n",
        "\n",
        "  helpfulness_response = helpfulness_chain.invoke({\"initial_query\" : initial_query.content, \"final_response\" : final_response.content})\n",
        "\n",
        "  if \"Y\" in helpfulness_response:\n",
        "    print(\"Helpful!\")\n",
        "    return \"end\"\n",
        "  else:\n",
        "    print(\"Not helpful!\")\n",
        "    return \"continue\"\n",
        "\n",
        "def dummy_node(state):\n",
        "  return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fz1u9Vf4SHxJ"
      },
      "source": [
        "####üèóÔ∏è Activity #4:\n",
        "\n",
        "Please write what is happening in our `check_helpfulness` function!\n",
        "\n",
        "### ANSWER\n",
        "\n",
        "- Check if there is more then 10 messages. IF so, end the convo.\n",
        "- Construct prompt template with the initial_query and final_response\n",
        "- Pass prompt using langchain to gpt-4 model to evalaute helpfulnes\n",
        "- Return Helpful! or Not helpful! depending on the qestion/answer pair "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sD7EV0HqSQcb"
      },
      "source": [
        "Now we can set our graph up! This process will be almost entirely the same - with the inclusion of one additional node/conditional edge!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oajBwLkFVi1N"
      },
      "source": [
        "####üèóÔ∏è Activity #5:\n",
        "\n",
        "Please write markdown for the following cells to explain what each is doing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6rN7feNVn9f"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "We create 3 nodes - agent(calls model), action(calls tool) and passthrough (doensn't do anything)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6r6XXA5FJbVf"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check = StateGraph(AgentState)\n",
        "\n",
        "graph_with_helpfulness_check.add_node(\"agent\", call_model)\n",
        "graph_with_helpfulness_check.add_node(\"action\", call_tool)\n",
        "graph_with_helpfulness_check.add_node(\"passthrough\", dummy_node)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZ22o2mWVrfp"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "Set the agent node as entry point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "HNWHwWxuRiLY"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BhnBW2YVsJO"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "We add conditional edges to the agent node. If should_continue returns continue, we go to action node. If it returns end, then go to passthrough.\n",
        "\n",
        "We add conditional edges to the passthrough node. If check_helpfulness is not helpfull then go back to agent node, otherwise go to end."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "aVTKnWMbP_8T"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : \"passthrough\"\n",
        "    }\n",
        ")\n",
        "\n",
        "graph_with_helpfulness_check.add_conditional_edges(\n",
        "    \"passthrough\",\n",
        "    check_helpfulness,\n",
        "    {\n",
        "        \"continue\" : \"agent\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGDLEWOIVtK0"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "We add edge between action and agent to return the flow to agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "cbDK2MbuREgU"
      },
      "outputs": [],
      "source": [
        "graph_with_helpfulness_check.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSI8AOaEVvT-"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "Compile the graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "oQldl8ERQ8lf"
      },
      "outputs": [],
      "source": [
        "agent_with_helpfulness_check = graph_with_helpfulness_check.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F67FGCMRVwGz"
      },
      "source": [
        "##### YOUR MARKDOWN HERE\n",
        "\n",
        "Call the graph and return the answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3oo8E-PRK1T",
        "outputId": "72096ca6-e78e-475a-dde5-fe076c5b776d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\n",
            "\n",
            "\n",
            "Agent Response: Let's break down each of these terms and individuals:\n",
            "\n",
            "### LoRA (Low-Rank Adaptation)\n",
            "LoRA, or Low-Rank Adaptation, is a technique used in machine learning to adapt pre-trained models to new tasks with minimal computational resources. The idea is to decompose the weight matrices of neural networks into low-rank matrices, which can significantly reduce the number of parameters and computational cost. This is particularly useful in scenarios where you need to fine-tune large models on specific tasks without having access to extensive computational resources.\n",
            "\n",
            "### Tim Dettmers\n",
            "Tim Dettmers is a researcher known for his work in the field of machine learning, particularly in the areas of efficient training and inference of large neural networks. He has contributed to the development of techniques that make it feasible to train and deploy large models on more modest hardware. His work often focuses on optimizing the computational aspects of machine learning to make advanced models more accessible and efficient.\n",
            "\n",
            "### Attention\n",
            "Attention is a mechanism in machine learning, particularly in the context of neural networks, that allows the model to focus on specific parts of the input data when making predictions. It was introduced to improve the performance of models on tasks like machine translation, where understanding the context and relationships between different parts of the input is crucial.\n",
            "\n",
            "The most well-known application of attention is in the Transformer architecture, which has become the foundation for many state-of-the-art models in natural language processing (NLP), such as BERT, GPT, and T5. The attention mechanism works by assigning different weights to different parts of the input data, allowing the model to \"attend\" to the most relevant parts when making predictions.\n",
            "\n",
            "Would you like more detailed information on any of these topics?\n"
          ]
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"Related to machine learning, what is LoRA? Also, who is Tim Dettmers? Also, what is Attention?\")]}\n",
        "\n",
        "messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "\n",
        "print_messages(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yVmZPs6lnpsM"
      },
      "source": [
        "### Task 4: LangGraph for the \"Patterns\" of GenAI\n",
        "\n",
        "Let's ask our system about the 4 patterns of Generative AI:\n",
        "\n",
        "1. Prompt Engineering\n",
        "2. RAG\n",
        "3. Fine-tuning\n",
        "4. Agents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "ZoLl7GlXoae-"
      },
      "outputs": [],
      "source": [
        "patterns = [\"prompt engineering\", \"RAG\", \"fine-tuning\", \"LLM-based agents\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zkh0YJuCp3Zl",
        "outputId": "1c29765b-42fc-449f-dce4-62dcb747a03b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Helpful!\n",
            "Initial Query: What is prompt engineering and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Prompt engineering is a concept primarily associated with the field of artificial intelligence (AI) and natural language processing (NLP). It involves the design and crafting of prompts or input queries to elicit specific responses or behaviors from AI models, particularly large language models like GPT-3, GPT-4, and others. The goal of prompt engineering is to optimize the interaction with these models to achieve desired outcomes, whether for generating text, answering questions, or performing other tasks.\n",
            "\n",
            "### Key Aspects of Prompt Engineering:\n",
            "1. **Crafting Effective Prompts**: Designing prompts that are clear, concise, and tailored to the model's strengths.\n",
            "2. **Iterative Testing**: Continuously refining prompts based on the responses received to improve accuracy and relevance.\n",
            "3. **Understanding Model Behavior**: Gaining insights into how different prompts influence the model's output.\n",
            "4. **Application-Specific Design**: Customizing prompts for specific applications, such as customer service, content creation, or educational tools.\n",
            "\n",
            "### Emergence of Prompt Engineering:\n",
            "Prompt engineering became more prominent with the advent of large-scale language models like OpenAI's GPT-3, which was released in June 2020. The capabilities of these models to generate human-like text based on prompts highlighted the importance of crafting effective prompts to harness their full potential. As these models became more widely used in various applications, the practice of prompt engineering gained traction as a critical skill for developers, researchers, and AI practitioners.\n",
            "\n",
            "Would you like more detailed information or specific examples of prompt engineering in practice?\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is RAG and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: RAG stands for Retrieval-Augmented Generation. It is a framework that combines the strengths of retrieval-based and generation-based models to improve the performance of natural language processing tasks, particularly in the context of question answering and information retrieval.\n",
            "\n",
            "### Key Components of RAG:\n",
            "1. **Retriever**: This component is responsible for fetching relevant documents or passages from a large corpus based on the input query.\n",
            "2. **Generator**: This component generates a coherent and contextually appropriate response based on the retrieved documents.\n",
            "\n",
            "### How RAG Works:\n",
            "1. **Query Input**: A user inputs a query.\n",
            "2. **Document Retrieval**: The retriever fetches relevant documents or passages from a pre-indexed corpus.\n",
            "3. **Response Generation**: The generator uses the retrieved documents to generate a response that is both relevant and contextually accurate.\n",
            "\n",
            "### Advantages of RAG:\n",
            "- **Improved Accuracy**: By leveraging external documents, RAG can provide more accurate and contextually relevant answers.\n",
            "- **Scalability**: It can handle large corpora of documents, making it suitable for applications requiring extensive knowledge bases.\n",
            "- **Flexibility**: It can be fine-tuned for specific domains or tasks, enhancing its versatility.\n",
            "\n",
            "### When Did RAG Break Onto the Scene?\n",
            "RAG was introduced by Facebook AI Research (FAIR) in a paper titled \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks,\" which was published in 2020. The paper demonstrated the effectiveness of RAG in various knowledge-intensive tasks, setting a new benchmark for performance in these areas.\n",
            "\n",
            "Would you like more detailed information or specific aspects of RAG?\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is fine-tuning and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: Fine-tuning is a process in machine learning and natural language processing where a pre-trained model is further trained on a specific task or dataset to improve its performance on that task. This involves taking a model that has already been trained on a large, general dataset and then training it on a smaller, task-specific dataset. The idea is to leverage the general knowledge the model has already acquired and adapt it to perform well on a more specific task.\n",
            "\n",
            "### Key Steps in Fine-Tuning:\n",
            "1. **Pre-training**: Train a model on a large, diverse dataset to learn general features and patterns.\n",
            "2. **Fine-tuning**: Further train the pre-trained model on a smaller, task-specific dataset to adapt it to the specific requirements of the task.\n",
            "\n",
            "### Benefits of Fine-Tuning:\n",
            "- **Efficiency**: Reduces the amount of data and computational resources needed compared to training a model from scratch.\n",
            "- **Performance**: Often leads to better performance on specific tasks because the model starts with a strong foundation of general knowledge.\n",
            "- **Flexibility**: Allows the same pre-trained model to be adapted for various tasks.\n",
            "\n",
            "### Historical Context:\n",
            "Fine-tuning became particularly prominent with the advent of large-scale pre-trained models in natural language processing (NLP). One of the key milestones was the introduction of the BERT (Bidirectional Encoder Representations from Transformers) model by Google in 2018. BERT demonstrated that fine-tuning a pre-trained model on specific tasks could achieve state-of-the-art results across a wide range of NLP benchmarks.\n",
            "\n",
            "Before BERT, models like ELMo (Embeddings from Language Models) and ULMFiT (Universal Language Model Fine-tuning) also contributed to the popularity of fine-tuning by showing its effectiveness in various NLP tasks. However, BERT's success and the subsequent development of models like GPT (Generative Pre-trained Transformer) by OpenAI solidified fine-tuning as a standard approach in the field.\n",
            "\n",
            "Would you like more detailed information on any specific aspect of fine-tuning?\n",
            "\n",
            "\n",
            "\n",
            "Helpful!\n",
            "Initial Query: What is LLM-based agents and when did it break onto the scene??\n",
            "\n",
            "\n",
            "Agent Response: LLM-based agents, or Large Language Model-based agents, are artificial intelligence systems that leverage large language models to perform a variety of tasks. These tasks can include natural language understanding, text generation, translation, summarization, question answering, and more. The core technology behind these agents is typically a deep learning model trained on vast amounts of text data to understand and generate human-like text.\n",
            "\n",
            "### Key Characteristics of LLM-based Agents:\n",
            "1. **Natural Language Processing (NLP):** They excel in understanding and generating human language.\n",
            "2. **Contextual Understanding:** They can maintain context over long conversations or documents.\n",
            "3. **Versatility:** They can be fine-tuned for specific tasks or domains.\n",
            "4. **Scalability:** They can handle a wide range of applications from chatbots to complex data analysis.\n",
            "\n",
            "### Breakthrough and Evolution:\n",
            "The concept of LLM-based agents has evolved over time, but significant breakthroughs can be traced back to a few key developments:\n",
            "\n",
            "1. **2018 - BERT (Bidirectional Encoder Representations from Transformers):** Developed by Google, BERT was one of the first models to achieve state-of-the-art results on a variety of NLP tasks by understanding the context of words in a sentence bidirectionally.\n",
            "\n",
            "2. **2018 - GPT (Generative Pre-trained Transformer):** OpenAI's GPT model introduced the idea of pre-training a language model on a large corpus of text and then fine-tuning it for specific tasks. GPT-2, released in 2019, further demonstrated the power of this approach with its ability to generate coherent and contextually relevant text.\n",
            "\n",
            "3. **2020 - GPT-3:** OpenAI's GPT-3, with 175 billion parameters, marked a significant leap in the capabilities of language models. It demonstrated an unprecedented ability to generate human-like text and perform a wide range of tasks with minimal fine-tuning.\n",
            "\n",
            "4. **2021 and Beyond - Continued Advancements:** Since GPT-3, there have been numerous advancements in LLMs, including models like T5, RoBERTa, and more specialized versions of GPT. These models have been integrated into various applications, from virtual assistants to automated content creation tools.\n",
            "\n",
            "### Applications:\n",
            "- **Customer Support:** Chatbots and virtual assistants that can handle customer queries.\n",
            "- **Content Creation:** Tools for generating articles, reports, and creative writing.\n",
            "- **Translation Services:** Real-time language translation.\n",
            "- **Data Analysis:** Summarizing and extracting insights from large datasets.\n",
            "- **Healthcare:** Assisting in medical diagnosis and patient interaction.\n",
            "\n",
            "In summary, LLM-based agents have become increasingly prominent since the late 2010s, with major breakthroughs occurring around 2018-2020. They continue to evolve, offering more sophisticated and versatile applications across various industries.\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for pattern in patterns:\n",
        "  what_is_string = f\"What is {pattern} and when did it break onto the scene??\"\n",
        "  inputs = {\"messages\" : [HumanMessage(content=what_is_string)]}\n",
        "  messages = agent_with_helpfulness_check.invoke(inputs)\n",
        "  print_messages(messages)\n",
        "  print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
